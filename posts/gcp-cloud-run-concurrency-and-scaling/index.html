<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Casey Haralson">
<meta name="dcterms.date" content="2024-01-02">
<meta name="description" content="A look at Google Cloud’s Cloud Run concurrency and scaling characteristics.">

<title>Cloud Report - Cloud Run Concurrency and Scaling Characteristics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Cloud Report - Cloud Run Concurrency and Scaling Characteristics">
<meta property="og:description" content="A look at Google Cloud’s Cloud Run concurrency and scaling characteristics.">
<meta property="og:image" content="https://blog.caseyharalson.com/posts/gcp-cloud-run-concurrency-and-scaling/thumbnail.jpg">
<meta property="og:site-name" content="Cloud Report">
<meta name="twitter:title" content="Cloud Report - Cloud Run Concurrency and Scaling Characteristics">
<meta name="twitter:description" content="A look at Google Cloud’s Cloud Run concurrency and scaling characteristics.">
<meta name="twitter:image" content="https://blog.caseyharalson.com/posts/gcp-cloud-run-concurrency-and-scaling/thumbnail.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Cloud Report</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/CaseyHaralson" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Cloud Run Concurrency and Scaling Characteristics</h1>
                  <div>
        <div class="description">
          A look at Google Cloud’s Cloud Run concurrency and scaling characteristics.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">gcp</div>
                <div class="quarto-category">serverless</div>
                <div class="quarto-category">cloud run</div>
                <div class="quarto-category">concurrency</div>
                <div class="quarto-category">scaling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Casey Haralson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 2, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#concurrency" id="toc-concurrency" class="nav-link" data-scroll-target="#concurrency">Concurrency</a></li>
  <li><a href="#scaling" id="toc-scaling" class="nav-link" data-scroll-target="#scaling">Scaling</a></li>
  <li><a href="#service-revision-changes" id="toc-service-revision-changes" class="nav-link" data-scroll-target="#service-revision-changes">Service Revision Changes</a></li>
  </ul></li>
  <li><a href="#concurrency-experiments" id="toc-concurrency-experiments" class="nav-link" data-scroll-target="#concurrency-experiments">Concurrency Experiments</a>
  <ul class="collapse">
  <li><a href="#max-concurrency-experiment" id="toc-max-concurrency-experiment" class="nav-link" data-scroll-target="#max-concurrency-experiment">Max Concurrency Experiment</a></li>
  <li><a href="#max-concurrency-experiment-verification" id="toc-max-concurrency-experiment-verification" class="nav-link" data-scroll-target="#max-concurrency-experiment-verification">Max Concurrency Experiment Verification</a></li>
  </ul></li>
  <li><a href="#scaling-experiments" id="toc-scaling-experiments" class="nav-link" data-scroll-target="#scaling-experiments">Scaling Experiments</a>
  <ul class="collapse">
  <li><a href="#single-instance-baseline-experiment" id="toc-single-instance-baseline-experiment" class="nav-link" data-scroll-target="#single-instance-baseline-experiment">Single Instance Baseline Experiment</a></li>
  <li><a href="#two-instances-experiment" id="toc-two-instances-experiment" class="nav-link" data-scroll-target="#two-instances-experiment">Two Instances Experiment</a></li>
  <li><a href="#three-instances-experiment" id="toc-three-instances-experiment" class="nav-link" data-scroll-target="#three-instances-experiment">Three Instances Experiment</a></li>
  <li><a href="#five-instances-experiment" id="toc-five-instances-experiment" class="nav-link" data-scroll-target="#five-instances-experiment">Five Instances Experiment</a></li>
  <li><a href="#ten-instances-experiment" id="toc-ten-instances-experiment" class="nav-link" data-scroll-target="#ten-instances-experiment">Ten Instances Experiment</a></li>
  <li><a href="#ten-warm-instances-experiment" id="toc-ten-warm-instances-experiment" class="nav-link" data-scroll-target="#ten-warm-instances-experiment">Ten Warm Instances Experiment</a></li>
  </ul></li>
  <li><a href="#revision-change-experiments" id="toc-revision-change-experiments" class="nav-link" data-scroll-target="#revision-change-experiments">Revision Change Experiments</a>
  <ul class="collapse">
  <li><a href="#single-instance-to-single-instance-revision-change" id="toc-single-instance-to-single-instance-revision-change" class="nav-link" data-scroll-target="#single-instance-to-single-instance-revision-change">Single Instance to Single Instance Revision Change</a></li>
  </ul></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  <li><a href="#comments" id="toc-comments" class="nav-link" data-scroll-target="#comments">Comments</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>In this article I’d like to focus on how Cloud Run handles concurrency and scaling. <strong>Concurrency</strong> will allow a single instance to do more work, and <strong>scaling</strong> will show how more instances can be used to do even more work when the situation arises.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>The methods build on the methods from <a href="../gcp-serverless-comparison#methods">this GCP service comparison article.</a> In summary:</p>
<ul>
<li>a Cloud Run service is loaded with a basic nodejs app that responds to web requests with “Hello, World!”</li>
<li>the service is configured with 1 CPU and 2 GB of memory</li>
<li>a Cloud Shell instance close to the service is used to generate web requests that slam the Cloud Run service</li>
</ul>
<p>In that article, the service was only allowed to handle one request at a time and only one instance was allowed to be running at a time. This gave a baseline for the service, but both of these two things are going to be changed during the experiments here.</p>
<section id="concurrency" class="level3">
<h3 class="anchored" data-anchor-id="concurrency">Concurrency</h3>
<p>The default level of concurrency for a Cloud Run service is set at 80 concurrent requests. This means that the load balancer sending requests to the service will send 80 requests at a time to the service before considering that the service is full. A service that is full will handle a request, return it, and only then will the load balancer send another request to the service.</p>
<p>The concurrency experiment here will be about setting the service at different levels of concurrency to see how it responds. My guess is that there will be a sweet spot of how many requests the service (running this Hello World app) can handle before the service overloads and the requests start taking longer to respond than would be ideal.</p>
<section id="load-generation" class="level4">
<h4 class="anchored" data-anchor-id="load-generation">Load Generation</h4>
<p>Hey: <a href="https://github.com/rakyll/hey">https://github.com/rakyll/hey</a></p>
<p>To test the request throughput and latency distribution, “Hey” will again be used to generate the request load against the service. Hey will be used with different levels of concurrency until 95% of requests are returned under 100 ms.</p>
</section>
</section>
<section id="scaling" class="level3">
<h3 class="anchored" data-anchor-id="scaling">Scaling</h3>
<p>Cloud Run instance autoscaling reference: <a href="https://cloud.google.com/run/docs/about-instance-autoscaling">https://cloud.google.com/run/docs/about-instance-autoscaling</a></p>
<p>I would like to answer several questions around scaling services with Cloud Run:</p>
<ul>
<li>How long do new instances take to spin up?</li>
<li>Does adding more instances allow for linearly more work to be done?</li>
<li>Given some scaling headroom, how many instances are spun up to handle some amount work?</li>
</ul>
<p>To answer the question about instance spin up time, the test will require the services to be “cold” (where no instances are ready to handle requests).</p>
<p>A single instance baseline will need to be tested first to get information about how a single instance spins up and how much throughput it can handle. Once this is known, this value can be used to guess how much work X instances will be able to handle. The experiments should then be able to check this guess against real data.</p>
<section id="load-generation-1" class="level4">
<h4 class="anchored" data-anchor-id="load-generation-1">Load Generation</h4>
<p>“Hey” will be used to generate load for this test as well.</p>
<p>Note: I ran into the maximum data that Hey can report which makes the later experiments’ data a bit incorrect. Hey can gather and report on 1,000,000 requests during a run and any requests after this will not be aggregated into the reported statistics. I checked the Hey source code to see if this same thing held true for the throughput reporting but it doesn’t seem to use the same mechanism for reporting. So the “Number of Requests that Timed Out” value will not be correct (as indicated with a “?”), but the request throughput should be accurate.</p>
</section>
</section>
<section id="service-revision-changes" class="level3">
<h3 class="anchored" data-anchor-id="service-revision-changes">Service Revision Changes</h3>
<p>What happens when a service is configured to run some code (A) but is then updated to run some new code (B)?</p>
<p>If the service is cold, then the other experiments in this article will tell us what would happen. But, if the service is currently performing work with the original code and then swapped over to the new code, these experiments won’t tell us what would happen.</p>
<p>This question and experiment seems related to the above, but is subtly different so needed its own section.</p>
<section id="changing-revisions" class="level4">
<h4 class="anchored" data-anchor-id="changing-revisions">Changing Revisions</h4>
<p>A service revision change can be caused by a change in the service configuration (max/min number of instances, max concurrency, etc) or in the code that the service should run.</p>
<p>In this case, to get good results from the experiment, nothing should change but the service should be tricked into thinking a new revision needs to be created. A new service revision can be created where it looks exactly like the previous revision and Cloud Run will swap all the traffic to the new revision.</p>
</section>
</section>
</section>
<section id="concurrency-experiments" class="level2">
<h2 class="anchored" data-anchor-id="concurrency-experiments">Concurrency Experiments</h2>
<p>The source code is available here: <a href="https://github.com/CaseyHaralson/cloud-report/tree/002/experiment/002_gcpCloudRunScaling">project setup and services creation code</a></p>
<p>Steps:</p>
<ol type="1">
<li><p>The Cloud Run service is loaded with the “hello world” app and set to 1 CPU, 2 GB of memory, and 1 max instance.</p></li>
<li><p>The service is set to the level of service concurrency that is being tested.</p></li>
<li><p>The service url is hit to make sure that the service is “warm” (active and ready to accept requests).</p></li>
<li><p>“Hey” is used to generate load for 5 minutes at the level of load concurrency that is being tested.</p>
<ul>
<li><code>hey -z 5m -c [hey concurrency] [service url]</code></li>
</ul></li>
</ol>
<section id="max-concurrency-experiment" class="level3">
<h3 class="anchored" data-anchor-id="max-concurrency-experiment">Max Concurrency Experiment</h3>
<p><a href="./results.html#cloud-run-max-concurrency-test">Result Screenshots</a></p>
<p>The Cloud Run configuration was changed to allow an instance to handle up to 80 concurrent requests. “Hey” was used to generate different levels of concurrency for 5 minute durations. This is still with a single instance of Cloud Run.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Request Throughput @ 80 Allowed Concurrent Requests</caption>
<colgroup>
<col style="width: 19%">
<col style="width: 16%">
<col style="width: 36%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Percent Requests Under 100 ms</th>
<th>Num Requests Timed Out</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>70</td>
<td>773</td>
<td>50%</td>
<td>17</td>
</tr>
<tr class="even">
<td>60</td>
<td>773</td>
<td>75%</td>
<td>0</td>
</tr>
<tr class="odd">
<td>50</td>
<td>757</td>
<td>75%</td>
<td>0</td>
</tr>
<tr class="even">
<td>40</td>
<td>771</td>
<td>90%</td>
<td>0</td>
</tr>
<tr class="odd">
<td><strong>30</strong></td>
<td><strong>772</strong></td>
<td><strong>95%</strong></td>
<td><strong>0</strong></td>
</tr>
<tr class="even">
<td>20</td>
<td>717</td>
<td>95%</td>
<td>0</td>
</tr>
<tr class="odd">
<td>10</td>
<td>644</td>
<td>99%</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><strong>30 concurrent requests</strong> seems to be the sweet spot for this setup. Above this number of concurrent requests and the request throughput doesn’t really change but the latency distribution changes to make more of the requests slower (for example, only 50% of requests being under 100ms).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/concurrency - throughput plot.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Plot of Concurrency vs Request Throughput</figcaption>
</figure>
</div>
<p>The screenshot below of the internal service request latency metrics also shows that the service (running our Hello World app) can’t consistently deal with concurrency higher than 30 requests at a time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/concurrency - latencies from 70 to 20 concurrency.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Cloud Run Metrics - 80 Max Instance Concurrency, Various Hey Concurrency - Request Latencies</figcaption>
</figure>
</div>
<p>The throughput results seem to be <strong>averaging around 770 requests per second</strong> which is 3.5 times higher than with single concurrency (219 requests per second). Also, the throughput results seem to be way more stable than with single concurrency which had a request throughput variability of <a href="../gcp-serverless-comparison#compiled-results">around 20%</a>.</p>
</section>
<section id="max-concurrency-experiment-verification" class="level3">
<h3 class="anchored" data-anchor-id="max-concurrency-experiment-verification">Max Concurrency Experiment Verification</h3>
<p><a href="./results.html#cloud-run-updated-max-concurrency-test">Result Screenshots</a></p>
<p>Cloud Run was updated to only allow for up to 30 concurrent requests (which was found as the sweet spot for this Hello World app). “Hey” was then used to generate requests above and below this amount to see how the service responded.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Request Throughput @ 30 Allowed Concurrent Requests</caption>
<colgroup>
<col style="width: 19%">
<col style="width: 16%">
<col style="width: 36%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Percent Requests Under 100 ms</th>
<th>Num Requests Timed Out</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>50</td>
<td>768</td>
<td>75%</td>
<td>8</td>
</tr>
<tr class="even">
<td>40</td>
<td>783</td>
<td>75%</td>
<td>0</td>
</tr>
<tr class="odd">
<td><strong>30</strong></td>
<td><strong>725</strong></td>
<td><strong>90%</strong></td>
<td><strong>0</strong></td>
</tr>
<tr class="even">
<td><strong>30 (test 2)</strong></td>
<td><strong>782</strong></td>
<td><strong>90%</strong></td>
<td><strong>0</strong></td>
</tr>
<tr class="odd">
<td>20</td>
<td>717</td>
<td>95%</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>30 concurrency seems to be close to the “correct” amount of instance concurrency. It wasn’t able to achieve 95% of requests under 100 ms, but under this level of concurrency and the request throughput dropped quite a bit. <strong>The ideal amount of concurrency for these tests would probably be around 25 - 27 service concurrency</strong>, but the current value will be close enough for these experiments.</p>
<p>Note: the test with 30 Hey concurrency was run twice. During the first run, the service internal metrics reported a really strange request latency metrics blip where some of the requests took a really long time. During the second run, the service internal metrics were within expected results and the throughput was more expected. A screenshot of the latency blip can be found below and more experiment screenshots can be found at the above “Result Screenshots” link.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/concurrency - weird latency blip.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Cloud Run Metrics - 30 Max Instance Concurrency, 30 Hey Concurrency, 1st Run - Weird Latency Blip</figcaption>
</figure>
</div>
</section>
</section>
<section id="scaling-experiments" class="level2">
<h2 class="anchored" data-anchor-id="scaling-experiments">Scaling Experiments</h2>
<p>The source code is available here: <a href="https://github.com/CaseyHaralson/cloud-report/tree/002/experiment/002_gcpCloudRunScaling">project setup and services creation code</a></p>
<p>Steps:</p>
<ol type="1">
<li><p>The Cloud Run service is loaded with the “hello world” app and set to 1 CPU, 2 GB of memory, and 30 max concurrency per instance (which was found in the above concurrency experiment).</p></li>
<li><p>The service is set to the number of max service instances that is being tested.</p></li>
<li><p>The internal service metrics are checked to make sure that the service is “cold” (no service instances are ready to process requests).</p></li>
<li><p>“Hey” is used to generate load for 5 minutes at the level of load concurrency that is being tested.</p>
<ul>
<li><code>hey -z 5m -c [hey concurrency] [service url]</code></li>
</ul></li>
</ol>
<section id="single-instance-baseline-experiment" class="level3">
<h3 class="anchored" data-anchor-id="single-instance-baseline-experiment">Single Instance Baseline Experiment</h3>
<p><a href="./results.html#single-cold-instance-baseline">Result Screenshots</a></p>
<p>To determine a baseline for scaling services, this service was configured for a single max instance and Hey was run with 30 concurrency. This experiment was run twice.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Cold Instance Request Throughput and Average Container Startup Time</caption>
<colgroup>
<col style="width: 6%">
<col style="width: 18%">
<col style="width: 32%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Run</th>
<th>Requests/sec</th>
<th>Num Requests Timed Out</th>
<th>Average Container Startup Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>676</td>
<td>17</td>
<td>1.49 seconds</td>
</tr>
<tr class="even">
<td>2</td>
<td>662</td>
<td>10</td>
<td>1.49 seconds</td>
</tr>
</tbody>
</table>
<p>A single cold instance with this setup had <strong>a throughput of around 669 requests per second</strong> and could start new instances in about 1.5 seconds.</p>
<p>This throughput is around 13% slower than a warm instance and had some requests that timed out.</p>
</section>
<section id="two-instances-experiment" class="level3">
<h3 class="anchored" data-anchor-id="two-instances-experiment">Two Instances Experiment</h3>
<p><a href="./results.html#two-instances">Result Screenshots</a></p>
<p>In this experiment, the number of max instances was raised to two.</p>
<p>First, “Hey” was run with 30 concurrent requests which a single instance could handle. Second, “Hey” was run with 60 concurrent requests which both instances should be able to handle together.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Two Instances Request Throughput and Average Container Startup Time</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 31%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Num Requests Timed Out</th>
<th>Average Container Startup Time</th>
<th>Num Instances</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>30</td>
<td>1,300</td>
<td>14</td>
<td>1.49 seconds</td>
<td>2</td>
</tr>
<tr class="even">
<td>60</td>
<td>1,458</td>
<td>27</td>
<td>0.41 seconds</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>Two instances should be able to handle 1,338 requests per second (based on the results from the single instance of 669 requests per second). <strong>The two instances were able to hit the calculated throughput.</strong></p>
<p>The average container startup time during the second test seems curious. Maybe the container instance was cached close to where the Cloud Run instance was being spun up?</p>
<p>During the first test with 30 Hey concurrency, two instances were spun up to handle the requests.</p>
</section>
<section id="three-instances-experiment" class="level3">
<h3 class="anchored" data-anchor-id="three-instances-experiment">Three Instances Experiment</h3>
<p><a href="./results.html#three-instances">Result Screenshots</a></p>
<p>In this experiment, the number of max instances was raised to three. The experiment was run three times, once with 30 Hey concurrency, once with 60, and lastly with 90.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Three Instances Request Throughput and Average Container Startup Time</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 31%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Num Requests Timed Out</th>
<th>Average Container Startup Time</th>
<th>Num Instances</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>30</td>
<td>1,854</td>
<td>14</td>
<td>1.23 seconds</td>
<td>3</td>
</tr>
<tr class="even">
<td>60</td>
<td>1,981</td>
<td>49</td>
<td>1.20 seconds</td>
<td>3</td>
</tr>
<tr class="odd">
<td>90</td>
<td>2,015</td>
<td>50</td>
<td>1.20 seconds</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Three instances should be able to handle 2,007 requests per second (669 x 3). <strong>The three instances were able to hit the calculated throughput.</strong></p>
<p>During the first test with 30 Hey concurrency, three instances were spun up to handle the requests.</p>
</section>
<section id="five-instances-experiment" class="level3">
<h3 class="anchored" data-anchor-id="five-instances-experiment">Five Instances Experiment</h3>
<p><a href="./results.html#five-instances">Result Screenshots</a></p>
<p>In this experiment, the number of max instances was raised to five. The experiment was run twice, once with 30 Hey concurrency and once with 150.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Five Instances Request Throughput and Average Container Startup Time</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 31%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Num Requests Timed Out</th>
<th>Average Container Startup Time</th>
<th>Num Instances</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>30</td>
<td>2,378</td>
<td>16</td>
<td>0.80 seconds</td>
<td>5</td>
</tr>
<tr class="even">
<td>150</td>
<td>3,389</td>
<td>52?</td>
<td>1.34 seconds</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>Five instances should be able to handle 3,345 requests per second (669 x 5). <strong>The five instances were able to hit the calculated throughput.</strong></p>
<p>Hey’s reporting limitations were hit during the test with 150 Hey concurrency. This issue was described above <a href="#load-generation-1">here</a>.</p>
<p>During the first test with 30 concurrency, five instances were spun up to handle the requests.</p>
</section>
<section id="ten-instances-experiment" class="level3">
<h3 class="anchored" data-anchor-id="ten-instances-experiment">Ten Instances Experiment</h3>
<p><a href="./results.html#ten-instances">Result Screenshots</a></p>
<p>In this experiment, the number of max instances was raised to ten. The experiment was run twice, once with 30 Hey concurrency and once with 300.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Ten Instances Request Throughput and Average Container Startup Time</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 31%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Num Requests Timed Out</th>
<th>Average Container Startup Time</th>
<th>Num Instances</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>30</td>
<td>2,287</td>
<td>25</td>
<td>0.93 seconds</td>
<td>9 then 5</td>
</tr>
<tr class="even">
<td>300</td>
<td>5,049</td>
<td>126?</td>
<td>0.39 seconds</td>
<td>10 - 11</td>
</tr>
<tr class="odd">
<td>300 (test 2)</td>
<td>5,660</td>
<td>87?</td>
<td>0.97 seconds</td>
<td>10</td>
</tr>
</tbody>
</table>
<p>Ten instances should be able to handle 6,690 requests per second (669 x 10). <strong>The ten instances were NOT able to hit the calculated throughput.</strong> The test with 300 Hey concurrency was run twice to see if the first run was abnormal. There was a lot of variability to the internal request latencies which makes it seem like there wasn’t enough time for the results to stabilize. Screenshots can be found in the above “Results Screenshots” link.</p>
<p>Hey’s reporting limitations were hit again during the tests with 300 Hey concurrency. This issue was described above <a href="#load-generation-1">here</a>.</p>
<p>This was the first time where the max number of instances weren’t spun up to handle the 30 concurrent Hey requests test. Only 9 instances were spun up to begin with and, over the course of the test, the number of instances were dropped to 5 active instances.</p>
</section>
<section id="ten-warm-instances-experiment" class="level3">
<h3 class="anchored" data-anchor-id="ten-warm-instances-experiment">Ten Warm Instances Experiment</h3>
<p><a href="./results.html#ten-instances-warm">Result Screenshots</a></p>
<p>Because of the variability in the above test with 10 cold instances, I wanted to see if more stable results could be achieved if the instances were warm.</p>
<p>In this experiment, 300 concurrent Hey requests were sent to ten warm instances instead of cold instances.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Ten Instances Request Throughput and Average Container Startup Time</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 31%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Num Requests Timed Out</th>
<th>Average Container Startup Time</th>
<th>Num Instances</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>300</td>
<td>7,162</td>
<td>1?</td>
<td>0.71 seconds</td>
<td>13 then 10</td>
</tr>
</tbody>
</table>
<p>Ten warm instances should have been able to handle 7,700 requests per second (770 requests per warm instance x 10), but the instances weren’t able to hit this number. <strong>The results from the warm test were off from the calculated result by 7% while the results from the cold test were off by 15% (best case).</strong> This suggests that ideal results can’t be achieved at higher scaling amounts. More work can still be done with more instances, but there is some attenuation.</p>
<p>Even though ten instances were warm and idling waiting for requests, the autoscaler started 3 more instances to handle the requests before scaling back to 10 instances.</p>
</section>
</section>
<section id="revision-change-experiments" class="level2">
<h2 class="anchored" data-anchor-id="revision-change-experiments">Revision Change Experiments</h2>
<p>The source code is available here: <a href="https://github.com/CaseyHaralson/cloud-report/tree/002/experiment/002_gcpCloudRunScaling">project setup and services creation code</a></p>
<p>Steps:</p>
<ol type="1">
<li><p>The Cloud Run service is loaded with the “hello world” app and set to 1 CPU, 2 GB of memory, 1 max instance, and 30 max concurrency.</p></li>
<li><p>The internal service metrics are checked to make sure that the service is “cold” (no service instances are ready to process requests).</p></li>
<li><p>“Hey” is used to generate load for 5 minutes at 30 concurrency.</p>
<ul>
<li><code>hey -z 5m -c 30 [service url]</code></li>
</ul></li>
<li><p>The service is tricked into creating a new revision in the middle of the test. Basically, the same code is deployed again with the same service settings to trigger a new revision but with no changes.</p></li>
</ol>
<section id="single-instance-to-single-instance-revision-change" class="level3">
<h3 class="anchored" data-anchor-id="single-instance-to-single-instance-revision-change">Single Instance to Single Instance Revision Change</h3>
<p><a href="./results.html#single-to-single-revision">Result Screenshots</a></p>
<p>In this experiment, a single cold instance is hit with Hey like normal, but then a revision is triggered in the middle of the test. Nothing in the instance will change, but the same code will be redeployed to Cloud Run which will make a new revision anyway.</p>
<table class="table-striped table-sm small table">
<caption>Cloud Run Cold Instance Revision Change Request Throughput and Average Container Startup Time</caption>
<colgroup>
<col style="width: 19%">
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Hey Concurrency</th>
<th>Requests/sec</th>
<th>Num Requests Timed Out</th>
<th>Average Container Startup Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>30</td>
<td>687</td>
<td>13</td>
<td>1.36 &amp; 1.02 seconds</td>
</tr>
<tr class="even">
<td>30 (test 2)</td>
<td>678</td>
<td>15</td>
<td>1.23 &amp; 0.93 seconds</td>
</tr>
</tbody>
</table>
<p>The original test of a single cold instance gave a throughput of around 669 requests per second (and 17ish requests timed out). In this test the request throughput ended up being around 682 requests per second. So <strong>changing a revision in the middle of a test didn’t really have much of an impact to request throughput.</strong></p>
<section id="internal-metrics" class="level4">
<h4 class="anchored" data-anchor-id="internal-metrics">Internal Metrics</h4>
<p>It was difficult to figure out when the new revision was loaded when looking at the metrics from the original test. Because of this, the second test was started exactly at 8:55:00 PM (concluding at 9:00:00 PM) and the second revision was loaded at 8:57:12 (as reported by the revision creation timestamp). This gave exact times that could be compared to the internal metrics.</p>
<p>The internal metrics start reporting a few minutes before the test started and for several minutes after the test ended. Also, the second revision’s instance isn’t shown to spin up when it actually spun up. This makes it complicated to see exactly what is happening on this timescale of minutes, but:</p>
<ul>
<li>the first revision’s instance was spun up and started handling requests</li>
<li>the second revision’s instance was spun up in the middle of the test and started handling requests</li>
<li>the first and second revision were both handling requests at the same time for just a bit of time (this didn’t happen for too long because the request throughput for this test wasn’t too much higher than a single instance handling all requests)</li>
<li>the first revision’s instance was spun down</li>
<li>the second revision’s instance finished handling the requests before spinning down</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/revision - instance count.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Cloud Run Metrics - 1 Cold Instance, 30 Concurrency, Revision Change Halfway - Instance Counts</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>It seems that more concurrency helps give consistency to a service’s throughput. There is still some variation, but the results are smoother.</p>
<p>Scaling seems to allow for linearly more work to be done. There is some tapering at higher scaling levels which could go away over longer durations, or this tapering will need to be taken into account when calculating the number of instances needed to do work. It also looks like more scaling allows for the instances to spin up faster.</p>
<p>There are a few things to note though:</p>
<ul>
<li>The autoscaler will scale up more instances than are required to do the work (5 instances are spun up but 1 instance could have done the work). Though, this will make the work complete faster and the request throughput better.</li>
<li>The autoscaler will sometimes spin up more instances than are configured to be spun up (spins up 13 when the max should be 10). So this will need to be kept in mind - and protections put in place - if the work that needs to be done should have a hard maximum (like only 1 instance can be running at a time).</li>
</ul>
<p>Overall, Cloud Run responds nicely with concurrency and scaling. These settings are very easy to change and the changes can be seen pretty much immediately. The internal metrics reporting gets a bit strange when looking at granular timescales, but should smooth out in larger aggregate timescales.</p>
</section>
<section id="comments" class="level2">
<h2 class="anchored" data-anchor-id="comments">Comments</h2>
<p>Leave a comment:</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="CaseyHaralson/blog" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>