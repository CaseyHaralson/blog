[
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "",
    "text": "Have you seen that LLMs (large language models like ChatGPT) are being used to summarize private data that can’t be searched for on the web? You can implement a few tricks and get a model like ChaptGPT to answer questions about or summarize a PDF, personal or company documents, or notes you’ve made over the years.\nThere are three basic steps:\n\nTurn your data into something that can be stored and searched through.\n\nData -&gt; Embedding Model -&gt; Vectors -&gt; Vector Database\n\nThink of something you want to know and ask for the relevant data.\n\nQuestion -&gt; Vector Database -&gt; Relevant Data\n\nSend the question and relevant data to a LLM and get it to answer the question.\n\nQuestion + Relevant Data -&gt; LLM -&gt; LLM Answers or Summarizes Data"
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#overview",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#overview",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "",
    "text": "Have you seen that LLMs (large language models like ChatGPT) are being used to summarize private data that can’t be searched for on the web? You can implement a few tricks and get a model like ChaptGPT to answer questions about or summarize a PDF, personal or company documents, or notes you’ve made over the years.\nThere are three basic steps:\n\nTurn your data into something that can be stored and searched through.\n\nData -&gt; Embedding Model -&gt; Vectors -&gt; Vector Database\n\nThink of something you want to know and ask for the relevant data.\n\nQuestion -&gt; Vector Database -&gt; Relevant Data\n\nSend the question and relevant data to a LLM and get it to answer the question.\n\nQuestion + Relevant Data -&gt; LLM -&gt; LLM Answers or Summarizes Data"
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#structured-vs-unstructured-data",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#structured-vs-unstructured-data",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Structured vs Unstructured Data",
    "text": "Structured vs Unstructured Data\nFirst, let’s talk about the sort of data that can be analyzed with this method.\nFor our purposes, there are two types of data: structured and unstructured data. Structured data is like data in a database or an excel document (think rows and columns). And, unstructured data is basically everything else like a word document, website page, or PDF.\nThe methods we are talking about in this article work best on unstructured data. LLMs have a bad habit of just making things up (hallucinating) and that doesn’t work very well, typically, when you want to know things about structured data.\nHow bad would it be if you wanted to know a sum, count, etc over some structured data and you received a made up answer? For example, it probably wouldn’t be good if you asked for your average bank account balance over time and got an answer that had no basis in fact. Let’s go make some decisions based on that answer, right?\nIn contrast, made up answers based on unstructured data tend to not be as bad, and it’s easier to verify if the answer was made up. Though, if made up answers WOULD be bad for your case, then you probably want someone trusted to be providing you with answers. You can still use this method to find the data that you are interested in, but you might not want to implement the last step which gets a summary or answer from the LLM based on your data."
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#embeddings",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#embeddings",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Embeddings",
    "text": "Embeddings\nEmbeddings are the underlying technology that makes searching data with this method work. So, what is an embedding?\nTo get a visual we can talk about, let’s take a few words from some sentences and plot them on a graph. “The quick brown fox jumps over the lazy dog” is plotted on the graph below in blue and the quote “Love isn’t a state of perfect caring, it is an active noun like struggle” from Mr. Rogers is plotted in green. I chose to not plot all the words just to make the example easier to see.\n\n\n\nPlot of “quick brown fox lazy dog” and “love perfect caring active struggle”\n\n\nThis is a simplified example of what is really going on, but see how there is a number next to each word on the graph? When we map a word to a number (fox to -.1 for example), this is an embedding. We have to do this mapping because computers are awesome with numbers and they aren’t really good with anything else.\nNot only are we converting a single word to a number, but similar words are embedded close to each other. See how fox, cat, dog, and rabbit (animals) are all close to each other (around -.2) and love, caring, and struggle (emotions) are close to each other (around .6)?\nNow, taking this to the next level… If we were to embed “yellow cat and red rabbit”, the embedding will end up visually closer to our “quick brown fox” sentence vs the Mr. Rogers quote. See how the blue and red lines overlap and are closer together than the green line in the plot below?\n\n\n\nPlot with addition of “yellow cat red rabbit”\n\n\nThis is some secret sauce! We can tell that “yellow cat red rabbit” is closer to “quick brown fox” than “love perfect caring” and none of the words in the phrases overlap! We can see on the graph that two of these things are close conceptually. This idea is going to allow us to search our data for concepts that are close to what we are looking for without having to know or match on an exact phrase."
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#embedding-model",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#embedding-model",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Embedding Model",
    "text": "Embedding Model\nGreat! We know what an embedding is, but how do we get them? Well, an embedding model is used to generate embeddings.\nA model in machine learning is basically a program that has been morphed to function in a particular way. You can make a model (program) that can tell if sentences use positive or negative language, if an image has particular objects in the scene, or any of the other wild stuff you might have heard about. The way the model has been morphed typically gives the model type its name (LLM, embedding model, etc).\nLet’s think of a model as a box with an open top and bottom. You can put something in the top of the box, the box does its magic, and drops the “answer” out of the bottom.\nThese sorts of machine learning models are trained (morphed) with these basic steps:\n\nhumans put something in the top of the box and tell the model what the answer should be (what should come out of the bottom of the box)\nthe model then gets trained (morphed) until it can reproduce the answer\nrepeat steps 1 and 2 a bunch of times with lots of different examples\nthe model can now guess what the answer should be when we DON’T tell it what the answer is for a particular input\nwhen we are impressed with the model’s answers, we can start using the model\n\nIn the case of our embedding model, we can put in a sentence, paragraph, article, etc and we get out embeddings. During the training, the model has learned what concepts are close to each other and can generate embeddings that can be compared like we saw in the plots above.\n\nExample Embedding Model\nThere are a bunch of models that can generate embeddings, but let’s use the current embedding model by OpenAI (the folks who made ChatGPT) as an example. We can give the model our input and it will return a bunch of numbers that look like this (only 14 numbers shown for brevity):\n[0.004021138, 0.035306297, -0.00503044, -0.019054601, -0.013294506, 0.023966111, -0.0010671605, 0.0017952082, -0.014104519, -0.033171978, 0.009128722, 0.016495988, 0.002171286, 0.0013315398, ...]\nI’m going to convert the “quick brown fox” embedding from the plot above to numbers so we can see they are similar (yet simplified) to what OpenAI’s embedding model dropped out:\n[0.1, -0.5, -0.1, 0.0, -0.3]"
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#vectors",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#vectors",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Vectors",
    "text": "Vectors\nThe embedding model dropped out a bunch of numbers, and this grouping of numbers is called a vector:\n[0.004021138, 0.035306297, -0.00503044, -0.019054601, -0.013294506, 0.023966111, -0.0010671605, 0.0017952082, -0.014104519, -0.033171978, 0.009128722, 0.016495988, 0.002171286, 0.0013315398, ...]\nA vector really is just a group of numbers. We are dealing with vectors that have been generated from an embedding model, so they are special in that the numbers represent concepts. But, ultimately a vector is just a group of numbers."
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#vector-database",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#vector-database",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Vector Database",
    "text": "Vector Database\nAt this point we have two things that we need to keep track of:\n\na piece of data (an article, PDF, notes, etc) (the input to the embedding model)\nthe vector that represents that data (the output from the embedding model)\n\nA vector database will allow us to store both of those things, and it will allow us to search through what we have stored. There are a bunch of vector databases that provide various amounts of functionality, but that is the gist of what they do.\n\nStoring Vector Data\nThe first thing the vector database allows us to do is to store our data.\nWe can take all of our articles/PDFs/notes/etc, convert them into vectors with an embedding model, and then save both into the vector database. This saves the data we actually care about (the article/PDF/notes) and ties it to the vector representation.\nSince the vector and data are tied together now, we can get from a vector to the underlying data that we actually care about. This is important when we start searching the vectors.\n\n\nSearching Vector Data\nThe second thing the vector database allows us to do is to search our data.\nWe can think of something we want to search for (“yellow cat red rabbit” for example), convert it into a vector, and ask the vector database to give us the closest results. The vector database searches the list of stored vectors, gathers a list of close vectors and the data tied to those vectors, and then returns the data we care about (the article/PDF/notes).\nSo, we will be able to hand a vector to a vector database and ask it to give us the closest 3 results, for example. And then the database will hand us back the 3 closest articles, PDFs, notes, or whatever it is that we have stored.\nThey can also generally tell us how close the question is to the results they are returning which is really useful.\nLet’s say that we are building this for a company and we are storing policy documents (or something like that). If we were to ask the vector database for the closest documents to some policy question we have, we would expect the results to be close to what we are asking. So, maybe a closeness score of 99%.\nBut, if we were to ask the vector database where the closest taco shop is, we would expect the results to be very unrelated. The database would still return us some results (from our earlier example, if “quick brown fox” wasn’t in our database and we asked for “yellow cat”, the database would still return “love perfect caring” as the closest result), but it wouldn’t be very close. So, maybe a closeness score of 20%?\nThis closeness score can then be used to determine if results should be returned to the user, or if we should just return “sorry, we don’t know about that”. We could set a threshold of 90%, maybe, to determine if the question being asked has anything to do with what we have stored."
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#steps-1-and-2",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#steps-1-and-2",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Steps 1 and 2",
    "text": "Steps 1 and 2\nAt this point we know enough to restate the first two steps that were laid out in the overview. The first two steps go like this:\n\nTurn your data into something that can be stored and searched through\n\ntake some unstructured data (from the example: our “quick brown fox” sentence and quote from Mr. Rogers)\npass it in to an embedding model that will embed similar information as close together\nthe embedding model will drop out a vector per piece of data that we pass in\nthe vectors and data can then be stored in a vector database\n\nThink of something you want to know and ask for the relevant data\n\ntake your question (from the example: “yellow cat red rabbit”)\npass it into the embedding model so it will embed the information in the same way as our data from step 1\nthe embedding model will drop out a vector\nask the vector database to return any data that is close to our question vector\n\n\nSuper cool, right?!\nWe have our data in a form that we can search, and we can find relevant data whenever we have something we want to find. This is a concept search and not a keyword search. So it doesn’t matter if what we are searching for has the same words as what we ask…the concepts just have to be close.\nBut, what if we wanted the computer to summarize the information or use the relevant data and answer our question? This is where retrieval augmented generation (RAG) comes in to play."
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#retrieval-augmented-generation-rag",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#retrieval-augmented-generation-rag",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Retrieval Augmented Generation (RAG)",
    "text": "Retrieval Augmented Generation (RAG)\nLarge language models (LLMs) like ChatGPT have been trained on a bunch of human conversations and concepts from sources like Wikipedia, Reddit, and anything public on the internet. This means that you can probably ask ChatGPT “what is the airspeed velocity of an unladen swallow”, and it will tell you that it is a quote from the movie Monty Python and the Holy Grail. Someone, and likely many someones, have talked about this quote online, and ChatGPT is able to figure this out.\nBut, ChatGPT has only been trained on PUBLIC data. This means that if you ask it a policy question about something specifically at your company, it won’t have any idea. It might still answer (and confidently!), but that is a different problem.\nWhat we can do, though, is give the LLM some context around our question and push it to answer from what we give it as context.\nThis is what retrieval augmented generation does. It implies that we have retrieved some data (like in step 2 from our method) and are augmenting the answer from the LLM to be based on the data we are giving it.\nLet’s talk about how we might do that.\n\nPrompting\nWhen you ask a LLM something, this is called a “prompt”. Not only can you directly ask it something, but you can add additional instructions to help drive the LLM’s output to what you want. For example, if I ask:\nHey ChatGPT, what is the airspeed velocity of an unladen swallow?\nCan you answer as a pirate might?\nthen ChatGPT might answer like (and it totally did answer like this):\nArrr matey! The airspeed velocity of an unladen swallow be a tricky matter, ye see. \nIf ye be talkin' 'bout a European swallow, it be 'round 20 miles per hour, but if ye be chattin' 'bout an African swallow, well, that be a whole different tale. \nPirates be knowin' their birds, but these winged critters be movin' faster than a ship in full sail, savvy? Yarrr!\nPrompts can be constructed in many different ways and they can get extremely complicated. Sometimes it can take a few tries (or a bunch of trial and error) to get the LLM to “understand” the prompt and give a good output.\nLet’s keep going with Monty Python and the Holy Grail and see if we can generate a prompt that can give us an answer based on a paragraph from Wikipedia. We can use the first paragraph from the “Development” section in the Wikipedia article and see if our heros ever got the grail. A prompt using this idea might look like the following code block.\nI grouped the “information” and the “question” into their own sections (as indicated by the &lt;information&gt; and &lt;question&gt; tags) and then told the LLM to use those sections.\nUse the following information to answer the question.\n\n&lt;information&gt;\nIn January 1973 the Monty Python troupe wrote the first draft of the screenplay.\nHalf of the material was set in the Middle Ages and half was set in the present day.\nThe group decided to focus on the Middle Ages, focusing on the legend of the Holy Grail. \nBy the fourth or fifth draft, the story was complete, and the cast joked that the fact that the Grail was never retrieved would be \"a big let-down ... a great anti-climax\". \nGraham Chapman said a challenge was incorporating scenes that did not fit the Holy Grail motif.\n&lt;/information&gt;\n\n&lt;question&gt;\nDid the heros in Monty Python and the Holy Grail ever get the grail?\n&lt;/question&gt;\nWe can copy and paste that exact thing into ChatGPT to see what it will do. This is what it returned for me:\nNo, the heroes in Monty Python and the Holy Grail did not retrieve the Holy Grail.\nThe cast joked that the fact that the Grail was never retrieved would be \"a big let-down ... a great anti-climax.\" \nSo, the film plays on this idea, and despite the characters' quest for the Holy Grail, they do not ultimately succeed in obtaining it.\nThis is great! The LLM answered from the information we provided. We now have a pattern we can use to pass in information and a question to a LLM to get an answer.\n\n\nSending Private Information to a Public LLM\nYou might be wondering if it is a good idea to send your private information to a public LLM. It probably isn’t. You don’t know what that company might be doing, but they are probably storing everything you send to the LLM.\nSome of these companies that make public LLMs also have private ones that won’t store your private information. These private LLMs have policies in place and are constructed in such a way as to not direct your information to their storage system. This is the way to go if you are worried about your private information being stored."
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#step-3",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#step-3",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Step 3",
    "text": "Step 3\nThis is it. We now know how to get a computer to answer a question we have about our data. Let’s restate the last step in this method:\n\nSend the question and relevant data to a LLM and get it to answer the question\n\nconstruct a prompt that includes the question and relevant data that we found in Step 2\nsend the prompt to a LLM\nthe LLM will spit out an answer\nand there was much rejoicing!"
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#final-thoughts",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#final-thoughts",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHopefully you walk away from this article with a pretty good idea of how you could store your data, search it for something relevant, and get a LLM to answer or summarize the result.\nWe talked about embeddings (which are super cool), a bit about how machine learning models are trained, vector databases, and retrieval augmented generation. While you might not be implementing something like this yourself, hopefully this gives you some ideas of what is possible.\nCheers!"
  },
  {
    "objectID": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#comments",
    "href": "posts/intro-to-embeddings-vector-databases-and-rag/index.html#comments",
    "title": "Intro to Embeddings, Vector Databases, and RAG",
    "section": "Comments",
    "text": "Comments\nLeave a comment:"
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "",
    "text": "In this article I’d like to focus on how Cloud Run handles concurrency and scaling. Concurrency will allow a single instance to do more work, and scaling will show how more instances can be used to do even more work when the situation arises."
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#overview",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#overview",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "",
    "text": "In this article I’d like to focus on how Cloud Run handles concurrency and scaling. Concurrency will allow a single instance to do more work, and scaling will show how more instances can be used to do even more work when the situation arises."
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#methods",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#methods",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "Methods",
    "text": "Methods\nThe methods build on the methods from this GCP service comparison article. In summary:\n\na Cloud Run service is loaded with a basic nodejs app that responds to web requests with “Hello, World!”\nthe service is configured with 1 CPU and 2 GB of memory\na Cloud Shell instance close to the service is used to generate web requests that slam the Cloud Run service\n\nIn that article, the service was only allowed to handle one request at a time and only one instance was allowed to be running at a time. This gave a baseline for the service, but both of these two things are going to be changed during the experiments here.\n\nConcurrency\nThe default level of concurrency for a Cloud Run service is set at 80 concurrent requests. This means that the load balancer sending requests to the service will send 80 requests at a time to the service before considering that the service is full. A service that is full will handle a request, return it, and only then will the load balancer send another request to the service.\nThe concurrency experiment here will be about setting the service at different levels of concurrency to see how it responds. My guess is that there will be a sweet spot of how many requests the service (running this Hello World app) can handle before the service overloads and the requests start taking longer to respond than would be ideal.\n\nLoad Generation\nHey: https://github.com/rakyll/hey\nTo test the request throughput and latency distribution, “Hey” will again be used to generate the request load against the service. Hey will be used with different levels of concurrency until 95% of requests are returned under 100 ms.\n\n\n\nScaling\nCloud Run instance autoscaling reference: https://cloud.google.com/run/docs/about-instance-autoscaling\nI would like to answer several questions around scaling services with Cloud Run:\n\nHow long do new instances take to spin up?\nDoes adding more instances allow for linearly more work to be done?\nGiven some scaling headroom, how many instances are spun up to handle some amount work?\n\nTo answer the question about instance spin up time, the test will require the services to be “cold” (where no instances are ready to handle requests).\nA single instance baseline will need to be tested first to get information about how a single instance spins up and how much throughput it can handle. Once this is known, this value can be used to guess how much work X instances will be able to handle. The experiments should then be able to check this guess against real data.\n\nLoad Generation\n“Hey” will be used to generate load for this test as well.\nNote: I ran into the maximum data that Hey can report which makes the later experiments’ data a bit incorrect. Hey can gather and report on 1,000,000 requests during a run and any requests after this will not be aggregated into the reported statistics. I checked the Hey source code to see if this same thing held true for the throughput reporting but it doesn’t seem to use the same mechanism for reporting. So the “Number of Requests that Timed Out” value will not be correct (as indicated with a “?”), but the request throughput should be accurate.\n\n\n\nService Revision Changes\nWhat happens when a service is configured to run some code (A) but is then updated to run some new code (B)?\nIf the service is cold, then the other experiments in this article will tell us what would happen. But, if the service is currently performing work with the original code and then swapped over to the new code, these experiments won’t tell us what would happen.\nThis question and experiment seems related to the above, but is subtly different so needed its own section.\n\nChanging Revisions\nA service revision change can be caused by a change in the service configuration (max/min number of instances, max concurrency, etc) or in the code that the service should run.\nIn this case, to get good results from the experiment, nothing should change but the service should be tricked into thinking a new revision needs to be created. A new service revision can be created where it looks exactly like the previous revision and Cloud Run will swap all the traffic to the new revision."
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#concurrency-experiments",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#concurrency-experiments",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "Concurrency Experiments",
    "text": "Concurrency Experiments\nThe source code is available here: project setup and services creation code\nSteps:\n\nThe Cloud Run service is loaded with the “hello world” app and set to 1 CPU, 2 GB of memory, and 1 max instance.\nThe service is set to the level of service concurrency that is being tested.\nThe service url is hit to make sure that the service is “warm” (active and ready to accept requests).\n“Hey” is used to generate load for 5 minutes at the level of load concurrency that is being tested.\n\nhey -z 5m -c [hey concurrency] [service url]\n\n\n\nMax Concurrency Experiment\nResult Screenshots\nThe Cloud Run configuration was changed to allow an instance to handle up to 80 concurrent requests. “Hey” was used to generate different levels of concurrency for 5 minute durations. This is still with a single instance of Cloud Run.\n\nCloud Run Request Throughput @ 80 Allowed Concurrent Requests\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nPercent Requests Under 100 ms\nNum Requests Timed Out\n\n\n\n\n70\n773\n50%\n17\n\n\n60\n773\n75%\n0\n\n\n50\n757\n75%\n0\n\n\n40\n771\n90%\n0\n\n\n30\n772\n95%\n0\n\n\n20\n717\n95%\n0\n\n\n10\n644\n99%\n0\n\n\n\n30 concurrent requests seems to be the sweet spot for this setup. Above this number of concurrent requests and the request throughput doesn’t really change but the latency distribution changes to make more of the requests slower (for example, only 50% of requests being under 100ms).\n\n\n\nPlot of Concurrency vs Request Throughput\n\n\nThe screenshot below of the internal service request latency metrics also shows that the service (running our Hello World app) can’t consistently deal with concurrency higher than 30 requests at a time.\n\n\n\nCloud Run Metrics - 80 Max Instance Concurrency, Various Hey Concurrency - Request Latencies\n\n\nThe throughput results seem to be averaging around 770 requests per second which is 3.5 times higher than with single concurrency (219 requests per second). Also, the throughput results seem to be way more stable than with single concurrency which had a request throughput variability of around 20%.\n\n\nMax Concurrency Experiment Verification\nResult Screenshots\nCloud Run was updated to only allow for up to 30 concurrent requests (which was found as the sweet spot for this Hello World app). “Hey” was then used to generate requests above and below this amount to see how the service responded.\n\nCloud Run Request Throughput @ 30 Allowed Concurrent Requests\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nPercent Requests Under 100 ms\nNum Requests Timed Out\n\n\n\n\n50\n768\n75%\n8\n\n\n40\n783\n75%\n0\n\n\n30\n725\n90%\n0\n\n\n30 (test 2)\n782\n90%\n0\n\n\n20\n717\n95%\n0\n\n\n\n30 concurrency seems to be close to the “correct” amount of instance concurrency. It wasn’t able to achieve 95% of requests under 100 ms, but under this level of concurrency and the request throughput dropped quite a bit. The ideal amount of concurrency for these tests would probably be around 25 - 27 service concurrency, but the current value will be close enough for these experiments.\nNote: the test with 30 Hey concurrency was run twice. During the first run, the service internal metrics reported a really strange request latency metrics blip where some of the requests took a really long time. During the second run, the service internal metrics were within expected results and the throughput was more expected. A screenshot of the latency blip can be found below and more experiment screenshots can be found at the above “Result Screenshots” link.\n\n\n\nCloud Run Metrics - 30 Max Instance Concurrency, 30 Hey Concurrency, 1st Run - Weird Latency Blip"
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#scaling-experiments",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#scaling-experiments",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "Scaling Experiments",
    "text": "Scaling Experiments\nThe source code is available here: project setup and services creation code\nSteps:\n\nThe Cloud Run service is loaded with the “hello world” app and set to 1 CPU, 2 GB of memory, and 30 max concurrency per instance (which was found in the above concurrency experiment).\nThe service is set to the number of max service instances that is being tested.\nThe internal service metrics are checked to make sure that the service is “cold” (no service instances are ready to process requests).\n“Hey” is used to generate load for 5 minutes at the level of load concurrency that is being tested.\n\nhey -z 5m -c [hey concurrency] [service url]\n\n\n\nSingle Instance Baseline Experiment\nResult Screenshots\nTo determine a baseline for scaling services, this service was configured for a single max instance and Hey was run with 30 concurrency. This experiment was run twice.\n\nCloud Run Cold Instance Request Throughput and Average Container Startup Time\n\n\n\n\n\n\n\n\nRun\nRequests/sec\nNum Requests Timed Out\nAverage Container Startup Time\n\n\n\n\n1\n676\n17\n1.49 seconds\n\n\n2\n662\n10\n1.49 seconds\n\n\n\nA single cold instance with this setup had a throughput of around 669 requests per second and could start new instances in about 1.5 seconds.\nThis throughput is around 13% slower than a warm instance and had some requests that timed out.\n\n\nTwo Instances Experiment\nResult Screenshots\nIn this experiment, the number of max instances was raised to two.\nFirst, “Hey” was run with 30 concurrent requests which a single instance could handle. Second, “Hey” was run with 60 concurrent requests which both instances should be able to handle together.\n\nCloud Run Two Instances Request Throughput and Average Container Startup Time\n\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nNum Requests Timed Out\nAverage Container Startup Time\nNum Instances\n\n\n\n\n30\n1,300\n14\n1.49 seconds\n2\n\n\n60\n1,458\n27\n0.41 seconds\n2\n\n\n\nTwo instances should be able to handle 1,338 requests per second (based on the results from the single instance of 669 requests per second). The two instances were able to hit the calculated throughput.\nThe average container startup time during the second test seems curious. Maybe the container instance was cached close to where the Cloud Run instance was being spun up?\nDuring the first test with 30 Hey concurrency, two instances were spun up to handle the requests.\n\n\nThree Instances Experiment\nResult Screenshots\nIn this experiment, the number of max instances was raised to three. The experiment was run three times, once with 30 Hey concurrency, once with 60, and lastly with 90.\n\nCloud Run Three Instances Request Throughput and Average Container Startup Time\n\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nNum Requests Timed Out\nAverage Container Startup Time\nNum Instances\n\n\n\n\n30\n1,854\n14\n1.23 seconds\n3\n\n\n60\n1,981\n49\n1.20 seconds\n3\n\n\n90\n2,015\n50\n1.20 seconds\n3\n\n\n\nThree instances should be able to handle 2,007 requests per second (669 x 3). The three instances were able to hit the calculated throughput.\nDuring the first test with 30 Hey concurrency, three instances were spun up to handle the requests.\n\n\nFive Instances Experiment\nResult Screenshots\nIn this experiment, the number of max instances was raised to five. The experiment was run twice, once with 30 Hey concurrency and once with 150.\n\nCloud Run Five Instances Request Throughput and Average Container Startup Time\n\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nNum Requests Timed Out\nAverage Container Startup Time\nNum Instances\n\n\n\n\n30\n2,378\n16\n0.80 seconds\n5\n\n\n150\n3,389\n52?\n1.34 seconds\n5\n\n\n\nFive instances should be able to handle 3,345 requests per second (669 x 5). The five instances were able to hit the calculated throughput.\nHey’s reporting limitations were hit during the test with 150 Hey concurrency. This issue was described above here.\nDuring the first test with 30 concurrency, five instances were spun up to handle the requests.\n\n\nTen Instances Experiment\nResult Screenshots\nIn this experiment, the number of max instances was raised to ten. The experiment was run twice, once with 30 Hey concurrency and once with 300.\n\nCloud Run Ten Instances Request Throughput and Average Container Startup Time\n\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nNum Requests Timed Out\nAverage Container Startup Time\nNum Instances\n\n\n\n\n30\n2,287\n25\n0.93 seconds\n9 then 5\n\n\n300\n5,049\n126?\n0.39 seconds\n10 - 11\n\n\n300 (test 2)\n5,660\n87?\n0.97 seconds\n10\n\n\n\nTen instances should be able to handle 6,690 requests per second (669 x 10). The ten instances were NOT able to hit the calculated throughput. The test with 300 Hey concurrency was run twice to see if the first run was abnormal. There was a lot of variability to the internal request latencies which makes it seem like there wasn’t enough time for the results to stabilize. Screenshots can be found in the above “Results Screenshots” link.\nHey’s reporting limitations were hit again during the tests with 300 Hey concurrency. This issue was described above here.\nThis was the first time where the max number of instances weren’t spun up to handle the 30 concurrent Hey requests test. Only 9 instances were spun up to begin with and, over the course of the test, the number of instances were dropped to 5 active instances.\n\n\nTen Warm Instances Experiment\nResult Screenshots\nBecause of the variability in the above test with 10 cold instances, I wanted to see if more stable results could be achieved if the instances were warm.\nIn this experiment, 300 concurrent Hey requests were sent to ten warm instances instead of cold instances.\n\nCloud Run Ten Instances Request Throughput and Average Container Startup Time\n\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nNum Requests Timed Out\nAverage Container Startup Time\nNum Instances\n\n\n\n\n300\n7,162\n1?\n0.71 seconds\n13 then 10\n\n\n\nTen warm instances should have been able to handle 7,700 requests per second (770 requests per warm instance x 10), but the instances weren’t able to hit this number. The results from the warm test were off from the calculated result by 7% while the results from the cold test were off by 15% (best case). This suggests that ideal results can’t be achieved at higher scaling amounts. More work can still be done with more instances, but there is some attenuation.\nEven though ten instances were warm and idling waiting for requests, the autoscaler started 3 more instances to handle the requests before scaling back to 10 instances."
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#revision-change-experiments",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#revision-change-experiments",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "Revision Change Experiments",
    "text": "Revision Change Experiments\nThe source code is available here: project setup and services creation code\nSteps:\n\nThe Cloud Run service is loaded with the “hello world” app and set to 1 CPU, 2 GB of memory, 1 max instance, and 30 max concurrency.\nThe internal service metrics are checked to make sure that the service is “cold” (no service instances are ready to process requests).\n“Hey” is used to generate load for 5 minutes at 30 concurrency.\n\nhey -z 5m -c 30 [service url]\n\nThe service is tricked into creating a new revision in the middle of the test. Basically, the same code is deployed again with the same service settings to trigger a new revision but with no changes.\n\n\nSingle Instance to Single Instance Revision Change\nResult Screenshots\nIn this experiment, a single cold instance is hit with Hey like normal, but then a revision is triggered in the middle of the test. Nothing in the instance will change, but the same code will be redeployed to Cloud Run which will make a new revision anyway.\n\nCloud Run Cold Instance Revision Change Request Throughput and Average Container Startup Time\n\n\n\n\n\n\n\n\nHey Concurrency\nRequests/sec\nNum Requests Timed Out\nAverage Container Startup Time\n\n\n\n\n30\n687\n13\n1.36 & 1.02 seconds\n\n\n30 (test 2)\n678\n15\n1.23 & 0.93 seconds\n\n\n\nThe original test of a single cold instance gave a throughput of around 669 requests per second (and 17ish requests timed out). In this test the request throughput ended up being around 682 requests per second. So changing a revision in the middle of a test didn’t really have much of an impact to request throughput.\n\nInternal Metrics\nIt was difficult to figure out when the new revision was loaded when looking at the metrics from the original test. Because of this, the second test was started exactly at 8:55:00 PM (concluding at 9:00:00 PM) and the second revision was loaded at 8:57:12 (as reported by the revision creation timestamp). This gave exact times that could be compared to the internal metrics.\nThe internal metrics start reporting a few minutes before the test started and for several minutes after the test ended. Also, the second revision’s instance isn’t shown to spin up when it actually spun up. This makes it complicated to see exactly what is happening on this timescale of minutes, but:\n\nthe first revision’s instance was spun up and started handling requests\nthe second revision’s instance was spun up in the middle of the test and started handling requests\nthe first and second revision were both handling requests at the same time for just a bit of time (this didn’t happen for too long because the request throughput for this test wasn’t too much higher than a single instance handling all requests)\nthe first revision’s instance was spun down\nthe second revision’s instance finished handling the requests before spinning down\n\n\n\n\nCloud Run Metrics - 1 Cold Instance, 30 Concurrency, Revision Change Halfway - Instance Counts"
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#final-thoughts",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#final-thoughts",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIt seems that more concurrency helps give consistency to a service’s throughput. There is still some variation, but the results are smoother.\nScaling seems to allow for linearly more work to be done. There is some tapering at higher scaling levels which could go away over longer durations, or this tapering will need to be taken into account when calculating the number of instances needed to do work. It also looks like more scaling allows for the instances to spin up faster.\nThere are a few things to note though:\n\nThe autoscaler will scale up more instances than are required to do the work (5 instances are spun up but 1 instance could have done the work). Though, this will make the work complete faster and the request throughput better.\nThe autoscaler will sometimes spin up more instances than are configured to be spun up (spins up 13 when the max should be 10). So this will need to be kept in mind - and protections put in place - if the work that needs to be done should have a hard maximum (like only 1 instance can be running at a time).\n\nOverall, Cloud Run responds nicely with concurrency and scaling. These settings are very easy to change and the changes can be seen pretty much immediately. The internal metrics reporting gets a bit strange when looking at granular timescales, but should smooth out in larger aggregate timescales."
  },
  {
    "objectID": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#comments",
    "href": "posts/gcp-cloud-run-concurrency-and-scaling/index.html#comments",
    "title": "Cloud Run Concurrency and Scaling Characteristics",
    "section": "Comments",
    "text": "Comments\nLeave a comment:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey there! I’m Casey, and this is a blog about computers and other things I happen to write about (I feel like I might write about more than just computers). I hope you find something interesting.\nDrop me a line if you want to chat about something.\nCheers!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud Report",
    "section": "",
    "text": "Intro to Embeddings, Vector Databases, and RAG\n\n\n\n\n\n\n\nvectors\n\n\nembeddings\n\n\ndatabase\n\n\nrag\n\n\nllm\n\n\nmachine learning\n\n\n\n\nAn introduction to using large language models to search and summarize your data.\n\n\n\n\n\n\nJan 21, 2024\n\n\nCasey Haralson\n\n\n\n\n\n\n  \n\n\n\n\nCloud Run Concurrency and Scaling Characteristics\n\n\n\n\n\n\n\ngcp\n\n\nserverless\n\n\ncloud run\n\n\nconcurrency\n\n\nscaling\n\n\n\n\nA look at Google Cloud’s Cloud Run concurrency and scaling characteristics.\n\n\n\n\n\n\nJan 2, 2024\n\n\nCasey Haralson\n\n\n\n\n\n\n  \n\n\n\n\nGCP Serverless Web Services Comparison\n\n\n\n\n\n\n\ngcp\n\n\nserverless\n\n\ncloud functions\n\n\ncloud run\n\n\napp engine\n\n\n\n\nA comparison between App Engine, Cloud Functions (1st and 2nd gen), and Cloud Run on Google Cloud.\n\n\n\n\n\n\nNov 28, 2023\n\n\nCasey Haralson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html",
    "href": "posts/gcp-serverless-comparison/index.html",
    "title": "GCP Serverless Web Services Comparison",
    "section": "",
    "text": "Updates:"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#overview",
    "href": "posts/gcp-serverless-comparison/index.html#overview",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Overview",
    "text": "Overview\nGoogle Cloud has several serverless offerings for handling web requests: App Engine, Cloud Functions, and Cloud Run. In this article I would like to compare the base request throughput between these offerings and explore the developer experience in working with these services.\nApp Engine is an easy way to create web applications without having to worry about things like databases, authentication, web servers, etc. This is an older service and probably shouldn’t be used for new projects. It is being included to see how things are evolving.\nCloud Functions runs specific code (a function) whenever a specified event happens. The function can be set to trigger on a web request or an event from a message bus. There are two different versions of Cloud Functions (1st and 2nd gen) and this article will compare both.\nCloud Run is a service that runs containerized code. This could be anything from a website to a machine learning model.\nServerless basically means that you don’t have to worry about hardware provisioning, software updating, etc. and you get flexible scalability. These services can scale to zero to minimize cost when no work needs to be done and can scale up “infinitely” when any amount of work needs to be done."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#methods",
    "href": "posts/gcp-serverless-comparison/index.html#methods",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Methods",
    "text": "Methods\nThe aim is to test each of the services in such a way that the only thing changing is the service being hit. The following items need to be the same (or as close as possible):\n\nthe code in the service\nthe service machine size\nthe service configuration\nthe app making the requests and source computer\n\n\nTest App\nI created a basic “hello world” app in nodejs for each of the different services where the app responds to web requests with “Hello, World!”. This should allow a baseline request throughput to be determined since the apps aren’t handling authentication, hitting external services, or doing any complex computation.\n\n\nServerless Parity\nThe app needs to be running at parity in each of the serverless services to eliminate differences in hardware and configuration from the throughput results. To get the services in parity, the memory, cpu, app concurrency, and app scaling amount need to be the same.\n\nBasics\nSince this is a nodejs app and nodejs is single threaded, only one cpu (max) is needed.\nCloud Functions 1st gen can only support one concurrent request at a time, so single concurrency should be chosen to allow for a fair comparison with the other services. Multiple concurrency can be experimented with at a different time.\nThis experiment is really to determine a baseline for these services, so app scaling should be minimized to only allow for a single instance. App scaling can be experimented on at a different time.\nThis leaves memory size and max CPU to be figured out for parity.\n\n\nCloud Run\nCloud Run configuration reference: https://cloud.google.com/run/docs/configuring/services/cpu\nCloud Run needs a minimum of 1 CPU to allow for more concurrency. This isn’t really helpful for this experiment, but it will be very useful in upcoming experiments. Lets choose 1 CPU which will give a better baseline for future experiments.\nBased on the selections for the other services, a memory size of “2 GiB” was chosen.\n\n\nCloud Functions\nCloud Functions configuration reference: https://cloud.google.com/functions/docs/configuring\nCloud Functions 1st gen only allows for a configuration of memory which then determines the CPU. Choosing “2048MB” for Cloud Functions 1st gen gives 1 CPU.\nA memory size of “2 GiB” and 1 CPU was chosen for Cloud Functions 2nd gen so it matches with the other services.\n\n\nApp Engine\nApp Engine vs Cloud Run configuration reference: https://cloud.google.com/appengine/migration-center/run/compare-gae-with-run\nChoosing an “Instance Class” for App Engine sets the memory and CPU. An instance class of “F4” sets 1 CPU and 1.5GB of memory which should be close enough for this experiment.\n\n\n\nRequest Throughput\nThere are a few ways that request throughput could be determined, but I was most interested in the end-user experience. This means that internal service metrics couldn’t be used and that a load generator of some sort should be used.\n\nHey\nHey: https://github.com/rakyll/hey\nI experimented with several programs that slam endpoints with requests, but ultimately ended up sticking with a program called “Hey”. I found that Hey’s output was easiest to see differences in the response latency distribution, and it could send a certain number of requests for quick testing or send requests for a specific amount of time for longer tests.\nThis program runs from one computer (destination) and hits the target server with requests. This will give us the request throughput that the server can deliver to the destination computer.\n\n\nLatency Distribution\n\n\n\nHey - Sample Output. 1. Shows the “requests per second”. 2. Shows the latency of requests per some amount of time. 3. Shows the number of successful responses.\n\n\nI played with several methods of trying to find request throughput, but ultimately ended up settling on trying to get 95% of the requests under 100 ms and then using that to find the request throughput. Aiming for 99% of the requests to be under a threshold gave too much variability while 95% was “fairly stable”.\n\n\nCalculating Throughput\nEven with targeting results where 95% of the requests come back in under 100 ms, there was still quite a bit of variability between the throughput in each run. I think the best that can be done is run a sustained load test multiple times to try and find an average with some error percentage.\nThe service should also be “warm” (one instance should already be active and ready to accept requests) so service startup times aren’t included in the request throughput calculations. Service startup times could be explored in another experiment.\n\n\nLoad Generating (Destination) Computer\nTo limit network effects from impacting the results, I chose to use a GCP Cloud Shell to generate the load. This puts the load generating computer within Google’s network and close to the services.\nCloud Shell Instance: https://shell.cloud.google.com/?show=terminal\nInstall Hey: sudo apt install hey\nTo check the Cloud Shell region/zone: curl -H \"Metadata-Flavor: Google\" metadata/computeMetadata/v1/instance/zone"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#experiment",
    "href": "posts/gcp-serverless-comparison/index.html#experiment",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Experiment",
    "text": "Experiment\nThe source code is available here: project setup and services creation code\nSteps:\n\nEach of the serverless services is loaded with, basically, the same “hello world” app. The services need slightly different boilerplate code, but ultimately the code we are testing is the same.\nEach of the serverless services is set to 1 CPU, 1 concurrency, 1 max instance, and around 2 GB of memory.\n“Hey” is used to quickly find the amount of concurrent requests that will lead to 95% of requests returning in under 100 ms. This is repeated until a semi stable result is found per service.\n\nhey -n 500 -c [concurrency] [service url]\n\n“Hey” is run for a longer duration (5 minutes) to find the throughput per service. This is repeated five times per service to find the average and error percentage.\n\nhey -z 5m -c [found concurrency] [service url]"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#experiment-results",
    "href": "posts/gcp-serverless-comparison/index.html#experiment-results",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Experiment Results",
    "text": "Experiment Results\n\nApp Engine\n\n\n\nApp Engine Results - First Run\n\n\n\nApp Engine Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n388\n\n\n2\n454\n\n\n3\n421\n\n\n4\n408\n\n\n5\n311\n\n\n\nApp Engine was able to achieve an average of 396 requests per second (+- 21%) with a Hey concurrency of 20 requests at a time.\n\n\nCloud Functions 1st Gen\n\n\n\nCloud Functions 1st Gen Results - First Run\n\n\n\nCloud Functions 1st Gen Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n286\n\n\n2\n301\n\n\n3\n301\n\n\n4\n298\n\n\n5\n305\n\n\n\nCloud Functions 1st gen was able to achieve an average of 298 requests per second (+- 4%) with a Hey concurrency of 15 requests at a time.\n\n\nCloud Functions 2nd Gen\n\n\n\nCloud Functions 2nd Gen Results - First Run\n\n\n\nCloud Functions 2nd Gen Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n190\n\n\n2\n219\n\n\n3\n219\n\n\n4\n221\n\n\n5\n216\n\n\n\nCloud Functions 2nd gen was able to achieve an average of 213 requests per second (+- 10%) with a Hey concurrency of 15 requests at a time.\n\n\nCloud Run\n\n\n\nCloud Run Results - First Run\n\n\n\nCloud Run Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n262\n\n\n2\n190\n\n\n3\n216\n\n\n4\n213\n\n\n5\n216\n\n\n\nCloud Run was able to achieve an average of 219 requests per second (+- 20%) with a Hey concurrency of 15 requests at a time.\n\n\nCompiled Results\nThese are the throughput results for each service at 1 CPU, 1 concurrency, 1 instance, and around 2 GB of memory:\n\nService Request Average Throughput, Deviation, and Rough Possible Range\n\n\nService\nAverage Requests/sec\n+- % Deviation\n~ Possible Range\n\n\n\n\nApp Engine\n396\n21%\n313 - 479 req/s\n\n\nCloud Functions 1st Gen\n298\n4%\n286 - 310 req/s\n\n\nCloud Run\n219\n20%\n175 - 263 req/s\n\n\nCloud Functions 2nd Gen\n213\n10%\n192 - 234 req/s"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#developer-experience",
    "href": "posts/gcp-serverless-comparison/index.html#developer-experience",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Developer Experience",
    "text": "Developer Experience\n\nApp Engine\nApp Engine gave the worst developer experience of the four services:\n\nTo enable App Engine, you must have “project owner” permissions.\nThe service can only be enabled once per project and in only one region. To enable the service in more than one region requires a project to be created per region.\nThe first app that is created in App Engine has to be called “default”. And this app can’t be deleted after creation, so Terraform can’t clean this up.\nApp Engine can’t be turned off or deleted after it is turned on in a project. The project has to be deleted to clean up App Engine.\nIn Terraform, you must include the runtime and entrypoint even though they are already defined in the app’s app.yaml file.\nIn Terraform, you have to manually version the app and this is difficult to automate.\nIn Terraform, there isn’t a way to get the app url. The app url has to be manually calculated with the App Engine’s location code being hardcoded.\n\nSome other notes about App Engine that are more neutral:\n\nThe apps are public to the internet by default. The other services (Cloud Functions and Cloud Run) had to have a role binding applied to allow anonymous access.\nGoogle has a comment in the App Engine vs Cloud Run comparison document that says they took what they learned from App Engine and put it into Cloud Run. And, that they made Cloud Run integrate with more services than with App Engine.\n\n\n\nCloud Functions\nThe Cloud Functions developer experience was pretty much exactly as expected (which is good). The following notes are things that stood out.\nCloud Functions 1st Gen:\n\nThere isn’t a way to control the CPU count directly. The CPU count is controlled by the amount of memory.\nEach function can only handle 1 concurrent request at a time.\n\nCloud Functions 2nd Gen:\n\nThe 2nd generation of Cloud Functions fixed the problems with the 1st gen.\nThe 2nd gen is actually using Cloud Run behind the scenes.\n\nCloud Functions (both generations) do have a potential negative, but I didn’t run into it during this experiment:\n\nEach function can only have one trigger. Multiple functions would have to be created to handle different triggering events even if the code to run was the same for each event.\n\n\n\nCloud Run\nCloud Run had the best developer experience. Since this service just runs a container, there is no special code or language required.\nThe only notes I would add deal with how the container is created and loaded into the Cloud Run service:\n\nThere is no way to submit code to Cloud Run directly from Terraform and no way to trigger Cloud Build to then submit the code to the service from Terraform. This typically wouldn’t be a problem in a normal development scenario because an external pipeline would be setup to maintain the service with the correct code. But, I didn’t want that overhead for these test experiments, so I had to create a script step to run the build and then deploy the container to Cloud Run after Terraform was applied.\nCloud Run can’t be created without an initial container image, and because of the specifics of this test environment, the initial container was set to a default Google container which was replaced later in a script step after Terraform was applied.\nCloud Build can’t be told where the Dockerfile is, so the app Dockerfile had to live in the root app folder instead of in a devops folder or somewhere more appropriate. There is a way to specify Dockerfile locations with a cloudbuild.yaml file, but this requires including special code (the cloudbuild.yaml file) which wasn’t appealing."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#final-thoughts",
    "href": "posts/gcp-serverless-comparison/index.html#final-thoughts",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI was shocked to find how inconsistent the throughput results were when considering just a single service. It was painful to find anything close to consistency and to get results that could be compared between services.\nIt was also interesting to see how each service compared when they were all dealing with the same limitation of single concurrency. I’m excited to see how each service handles concurrency and scaling."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#comments",
    "href": "posts/gcp-serverless-comparison/index.html#comments",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Comments",
    "text": "Comments\nLeave a comment:"
  }
]