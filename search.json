[
  {
    "objectID": "posts/gcp-serverless-comparison/index.html",
    "href": "posts/gcp-serverless-comparison/index.html",
    "title": "GCP Serverless Web Services Comparison",
    "section": "",
    "text": "Updates:"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#overview",
    "href": "posts/gcp-serverless-comparison/index.html#overview",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Overview",
    "text": "Overview\nGoogle Cloud has several serverless offerings for handling web requests: App Engine, Cloud Functions, and Cloud Run. In this article I would like to compare the base request throughput between these offerings and explore the developer experience in working with these services.\nApp Engine is an easy way to create web applications without having to worry about things like databases, authentication, web servers, etc. This is an older service and probably shouldn’t be used for new projects. It is being included to see how things are evolving.\nCloud Functions runs specific code (a function) whenever a specified event happens. The function can be set to trigger on a web request or an event from a message bus. There are two different versions of Cloud Functions (1st and 2nd gen) and this article will compare both.\nCloud Run is a service that runs containerized code. This could be anything from a website to a machine learning model.\nServerless basically means that you don’t have to worry about hardware provisioning, software updating, etc. and you get flexible scalability. These services can scale to zero to minimize cost when no work needs to be done and can scale up “infinitely” when any amount of work needs to be done."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#methods",
    "href": "posts/gcp-serverless-comparison/index.html#methods",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Methods",
    "text": "Methods\nThe aim is to test each of the services in such a way that the only thing changing is the service being hit. The following items need to be the same (or as close as possible):\n\nthe code in the service\nthe service machine size\nthe service configuration\nthe app making the requests and source computer\n\n\nTest App\nI created a basic “hello world” app in nodejs for each of the different services where the app responds to web requests with “Hello, World!”. This should allow a baseline request throughput to be determined since the apps aren’t handling authentication, hitting external services, or doing any complex computation.\n\n\nServerless Parity\nThe app needs to be running at parity in each of the serverless services to eliminate differences in hardware and configuration from the throughput results. To get the services in parity, the memory, cpu, app concurrency, and app scaling amount need to be the same.\n\nBasics\nSince this is a nodejs app and nodejs is single threaded, only one cpu (max) is needed.\nCloud Functions 1st gen can only support one concurrent request at a time, so single concurrency should be chosen to allow for a fair comparison with the other services. Multiple concurrency can be experimented with at a different time.\nThis experiment is really to determine a baseline for these services, so app scaling should be minimized to only allow for a single instance. App scaling can be experimented on at a different time.\nThis leaves memory size and max CPU to be figured out for parity.\n\n\nCloud Run\nCloud Run configuration reference: https://cloud.google.com/run/docs/configuring/services/cpu\nCloud Run needs a minimum of 1 CPU to allow for more concurrency. This isn’t really helpful for this experiment, but it will be very useful in upcoming experiments. Lets choose 1 CPU which will give a better baseline for future experiments.\nBased on the selections for the other services, a memory size of “2 GiB” was chosen.\n\n\nCloud Functions\nCloud Functions configuration reference: https://cloud.google.com/functions/docs/configuring\nCloud Functions 1st gen only allows for a configuration of memory which then determines the CPU. Choosing “2048MB” for Cloud Functions 1st gen gives 1 CPU.\nA memory size of “2 GiB” and 1 CPU was chosen for Cloud Functions 2nd gen so it matches with the other services.\n\n\nApp Engine\nApp Engine vs Cloud Run configuration reference: https://cloud.google.com/appengine/migration-center/run/compare-gae-with-run\nChoosing an “Instance Class” for App Engine sets the memory and CPU. An instance class of “F4” sets 1 CPU and 1.5GB of memory which should be close enough for this experiment.\n\n\n\nRequest Throughput\nThere are a few ways that request throughput could be determined, but I was most interested in the end-user experience. This means that internal service metrics couldn’t be used and that a load generator of some sort should be used.\n\nHey\nHey: https://github.com/rakyll/hey\nI experimented with several programs that slam endpoints with requests, but ultimately ended up sticking with a program called “Hey”. I found that Hey’s output was easiest to see differences in the response latency distribution, and it could send a certain number of requests for quick testing or send requests for a specific amount of time for longer tests.\nThis program runs from one computer (destination) and hits the target server with requests. This will give us the request throughput that the server can deliver to the destination computer.\n\n\nLatency Distribution\n\n\n\nHey - Sample Output. 1. Shows the “requests per second”. 2. Shows the latency of requests per some amount of time. 3. Shows the number of successful responses.\n\n\nI played with several methods of trying to find request throughput, but ultimately ended up settling on trying to get 95% of the requests under 100 ms and then using that to find the request throughput. Aiming for 99% of the requests to be under a threshold gave too much variability while 95% was “fairly stable”.\n\n\nCalculating Throughput\nEven with targeting results where 95% of the requests come back in under 100 ms, there was still quite a bit of variability between the throughput in each run. I think the best that can be done is run a sustained load test multiple times to try and find an average with some error percentage.\nThe service should also be “warm” (one instance should already be active and ready to accept requests) so service startup times aren’t included in the request throughput calculations. Service startup times could be explored in another experiment.\n\n\nLoad Generating (Destination) Computer\nTo limit network effects from impacting the results, I chose to use a GCP Cloud Shell to generate the load. This puts the load generating computer within Google’s network and close to the services.\nCloud Shell Instance: https://shell.cloud.google.com/?show=terminal\nInstall Hey: sudo apt install hey\nTo check the Cloud Shell region/zone: curl -H \"Metadata-Flavor: Google\" metadata/computeMetadata/v1/instance/zone"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#experiment",
    "href": "posts/gcp-serverless-comparison/index.html#experiment",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Experiment",
    "text": "Experiment\nThe source code is available here: project setup and services creation code\nSteps:\n\nEach of the serverless services is loaded with, basically, the same “hello world” app. The services need slightly different boilerplate code, but ultimately the code we are testing is the same.\nEach of the serverless services is set to 1 CPU, 1 concurrency, 1 max instance, and around 2 GB of memory.\n“Hey” is used to quickly find the amount of concurrent requests that will lead to 95% of requests returning in under 100 ms. This is repeated until a semi stable result is found per service.\n\nhey -n 500 -c [concurrency] [service url]\n\n“Hey” is run for a longer duration (5 minutes) to find the throughput per service. This is repeated five times per service to find the average and error percentage.\n\nhey -z 5m -c [found concurrency] [service url]"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#experiment-results",
    "href": "posts/gcp-serverless-comparison/index.html#experiment-results",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Experiment Results",
    "text": "Experiment Results\n\nApp Engine\n\n\n\nApp Engine Results - First Run\n\n\n\nApp Engine Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n388\n\n\n2\n454\n\n\n3\n421\n\n\n4\n408\n\n\n5\n311\n\n\n\nApp Engine was able to achieve an average of 396 requests per second (+- 21%) with a Hey concurrency of 20 requests at a time.\n\n\nCloud Functions 1st Gen\n\n\n\nCloud Functions 1st Gen Results - First Run\n\n\n\nCloud Functions 1st Gen Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n286\n\n\n2\n301\n\n\n3\n301\n\n\n4\n298\n\n\n5\n305\n\n\n\nCloud Functions 1st gen was able to achieve an average of 298 requests per second (+- 4%) with a Hey concurrency of 15 requests at a time.\n\n\nCloud Functions 2nd Gen\n\n\n\nCloud Functions 2nd Gen Results - First Run\n\n\n\nCloud Functions 2nd Gen Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n190\n\n\n2\n219\n\n\n3\n219\n\n\n4\n221\n\n\n5\n216\n\n\n\nCloud Functions 2nd gen was able to achieve an average of 213 requests per second (+- 10%) with a Hey concurrency of 15 requests at a time.\n\n\nCloud Run\n\n\n\nCloud Run Results - First Run\n\n\n\nCloud Run Throughput Results\n\n\nRun\nRequests/sec\n\n\n\n\n1\n262\n\n\n2\n190\n\n\n3\n216\n\n\n4\n213\n\n\n5\n216\n\n\n\nCloud Run was able to achieve an average of 219 requests per second (+- 20%) with a Hey concurrency of 15 requests at a time.\n\n\nCompiled Results\nThese are the throughput results for each service at 1 CPU, 1 concurrency, 1 instance, and around 2 GB of memory:\n\nService Request Average Throughput, Deviation, and Rough Possible Range\n\n\nService\nAverage Requests/sec\n+- % Deviation\n~ Possible Range\n\n\n\n\nApp Engine\n396\n21%\n313 - 479 req/s\n\n\nCloud Functions 1st Gen\n298\n4%\n286 - 310 req/s\n\n\nCloud Run\n219\n20%\n175 - 263 req/s\n\n\nCloud Functions 2nd Gen\n213\n10%\n192 - 234 req/s"
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#developer-experience",
    "href": "posts/gcp-serverless-comparison/index.html#developer-experience",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Developer Experience",
    "text": "Developer Experience\n\nApp Engine\nApp Engine gave the worst developer experience of the four services:\n\nTo enable App Engine, you must have “project owner” permissions.\nThe service can only be enabled once per project and in only one region. To enable the service in more than one region requires a project to be created per region.\nThe first app that is created in App Engine has to be called “default”. And this app can’t be deleted after creation, so Terraform can’t clean this up.\nApp Engine can’t be turned off or deleted after it is turned on in a project. The project has to be deleted to clean up App Engine.\nIn Terraform, you must include the runtime and entrypoint even though they are already defined in the app’s app.yaml file.\nIn Terraform, you have to manually version the app and this is difficult to automate.\nIn Terraform, there isn’t a way to get the app url. The app url has to be manually calculated with the App Engine’s location code being hardcoded.\n\nSome other notes about App Engine that are more neutral:\n\nThe apps are public to the internet by default. The other services (Cloud Functions and Cloud Run) had to have a role binding applied to allow anonymous access.\nGoogle has a comment in the App Engine vs Cloud Run comparison document that says they took what they learned from App Engine and put it into Cloud Run. And, that they made Cloud Run integrate with more services than with App Engine.\n\n\n\nCloud Functions\nThe Cloud Functions developer experience was pretty much exactly as expected (which is good). The following notes are things that stood out.\nCloud Functions 1st Gen:\n\nThere isn’t a way to control the CPU count directly. The CPU count is controlled by the amount of memory.\nEach function can only handle 1 concurrent request at a time.\n\nCloud Functions 2nd Gen:\n\nThe 2nd generation of Cloud Functions fixed the problems with the 1st gen.\nThe 2nd gen is actually using Cloud Run behind the scenes.\n\nCloud Functions (both generations) do have a potential negative, but I didn’t run into it during this experiment:\n\nEach function can only have one trigger. Multiple functions would have to be created to handle different triggering events even if the code to run was the same for each event.\n\n\n\nCloud Run\nCloud Run had the best developer experience. Since this service just runs a container, there is no special code or language required.\nThe only notes I would add deal with how the container is created and loaded into the Cloud Run service:\n\nThere is no way to submit code to Cloud Run directly from Terraform and no way to trigger Cloud Build to then submit the code to the service from Terraform. This typically wouldn’t be a problem in a normal development scenario because an external pipeline would be setup to maintain the service with the correct code. But, I didn’t want that overhead for these test experiments, so I had to create a script step to run the build and then deploy the container to Cloud Run after Terraform was applied.\nCloud Run can’t be created without an initial container image, and because of the specifics of this test environment, the initial container was set to a default Google container which was replaced later in a script step after Terraform was applied.\nCloud Build can’t be told where the Dockerfile is, so the app Dockerfile had to live in the root app folder instead of in a devops folder or somewhere more appropriate. There is a way to specify Dockerfile locations with a cloudbuild.yaml file, but this requires including special code (the cloudbuild.yaml file) which wasn’t appealing."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#final-thoughts",
    "href": "posts/gcp-serverless-comparison/index.html#final-thoughts",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI was shocked to find how inconsistent the throughput results were when considering just a single service. It was painful to find anything close to consistency and to get results that could be compared between services.\nIt was also interesting to see how each service compared when they were all dealing with the same limitation of single concurrency. I’m excited to see how each service handles concurrency and scaling."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#comments",
    "href": "posts/gcp-serverless-comparison/index.html#comments",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Comments",
    "text": "Comments\nLeave a comment:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey there! I’m Casey, and this is a blog about computers and other things I happen to write about."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud Report",
    "section": "",
    "text": "GCP Serverless Web Services Comparison\n\n\n\n\n\n\n\ngcp\n\n\nserverless\n\n\ncloud functions\n\n\ncloud run\n\n\napp engine\n\n\n\n\nA comparison between App Engine, Cloud Functions (1st and 2nd gen), and Cloud Run on Google Cloud.\n\n\n\n\n\n\nNov 28, 2023\n\n\nCasey Haralson\n\n\n\n\n\n\nNo matching items"
  }
]