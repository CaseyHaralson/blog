[
  {
    "objectID": "posts/gcp-serverless-comparison/index.html",
    "href": "posts/gcp-serverless-comparison/index.html",
    "title": "GCP Serverless Web Services Comparison",
    "section": "",
    "text": "Google Cloud has several serverless offerings for handling web requests: App Engine, Cloud Functions, and Cloud Run. In this article I would like to compare the request throughput between these offerings and explore the developer experience in working with these offerings.\nApp Engine is an easy way to create web applications without having to worry about things like databases, authentication, web servers, etc.\nCloud Functions runs specific code (a function) whenever a specified event happens. You can set the function to trigger on a web request or an event from a message bus. There are also two different versions of Cloud Functions: 1st gen and 2nd gen. This article will compare both.\nCloud Run is a service that runs containerized code. This could be anything from a website to a machine learning model.\nServerless basically means that you don’t have to worry about hardware provisioning, software updating, etc. and you get flexible scalability. These services can scale to zero to minimize cost when no work needs to be done and can scale up “infinitely” when any amount of work needs to be done."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#overview",
    "href": "posts/gcp-serverless-comparison/index.html#overview",
    "title": "GCP Serverless Web Services Comparison",
    "section": "",
    "text": "Google Cloud has several serverless offerings for handling web requests: App Engine, Cloud Functions, and Cloud Run. In this article I would like to compare the request throughput between these offerings and explore the developer experience in working with these offerings.\nApp Engine is an easy way to create web applications without having to worry about things like databases, authentication, web servers, etc.\nCloud Functions runs specific code (a function) whenever a specified event happens. You can set the function to trigger on a web request or an event from a message bus. There are also two different versions of Cloud Functions: 1st gen and 2nd gen. This article will compare both.\nCloud Run is a service that runs containerized code. This could be anything from a website to a machine learning model.\nServerless basically means that you don’t have to worry about hardware provisioning, software updating, etc. and you get flexible scalability. These services can scale to zero to minimize cost when no work needs to be done and can scale up “infinitely” when any amount of work needs to be done."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#methods",
    "href": "posts/gcp-serverless-comparison/index.html#methods",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Methods",
    "text": "Methods\nThe aim is to test each of the services in such a way that the only thing changing is the service being hit. The following items need to be the same (or as close as possible):\n\nthe code in the service\nthe service machine size\nthe service configuration\nthe app making the requests and source computer\n\n\nTest App\nI created a basic “hello world” app in nodejs for each of the different services where the app responds to web requests with “Hello, World!”. This should allow a baseline request throughput to be determined since the apps aren’t handling authentication, hitting external services, or doing any complex computation.\n\n\nServerless Parity\nThe app needs to be running at parity in each of the serverless services to eliminate differences in hardware and configuration from the throughput results. To get the services in parity, the memory, cpu, app concurrency, and app scaling amount need to be the same.\n\nBasics\nSince this is a nodejs app and nodejs is single threaded, only one cpu (max) is needed. Also, since the app doesn’t do any waiting on external services, concurrency wouldn’t help. This means that only single concurrency is needed (concurrency meaning how many requests can be handled by the app at the same time).\nThis experiment is really to determine a baseline for these services, so app scaling should be minimized to only allow for a single instance. App scaling can be experimented on at a different time.\nThis leaves memory size and max CPU to be figured out for parity.\n\n\nCloud Run\nCloud Run configuration reference: https://cloud.google.com/run/docs/configuring/services/cpu\nCloud Run needs a minimum of 1 CPU to allow for more concurrency. This isn’t really helpful for this experiment, but it will be very useful in upcoming experiments. Lets choose 1 CPU which will give a better baseline for future experiments.\nBased on the selections for the other services, a memory size of “2 GiB” will be chosen.\n\n\nCloud Functions\nCloud Functions configuration reference: https://cloud.google.com/functions/docs/configuring\nCloud Functions 1st gen only allows for a configuration of memory which then determines the CPU. Choosing “2048MB” for Cloud Functions 1st gen gives 1 CPU.\nA memory size of “2 GiB” and 1 CPU was chosen for Cloud Functions 2nd gen.\n\n\nApp Engine\nApp Engine vs Cloud Run configuration reference: https://cloud.google.com/appengine/migration-center/run/compare-gae-with-run\nChoosing an “Instance Class” for App Engine sets the memory and CPU. An instance class of “F4” sets 1 CPU and 1.5GB of memory which should be close enough for this experiment.\n\n\n\nRequest Throughput\nThere are many ways that request throughput could be determined, but I was most interested in the end-user experience. So, my decisions stemmed from that perspective.\nI would like to hear your thoughts if you would have made different decisions or if you have a different method for determining baselines.\n\nArtillery vs Hey\nArtillery: https://www.artillery.io/docs\nHey: https://github.com/rakyll/hey\nI experimented with two different programs that slam endpoints with requests: Artillery and Hey. These programs are run from the computer and location where you want to see response and latency. Both of them worked for this experiment, but I found that Hey’s output was easier to see differences in the response latency distribution.\n\n\nLatency Distribution\n\n\n\nHey - Sample Output. 1. Shows the “requests per second”. 2. Shows the latency of requests per some amount of time. 3. Shows the number of successful responses.\n\n\nI played with several methods of trying to find request throughput, but ultimately I ended up settling on trying to get 95% of the requests under 100 ms and then using that to find the request throughput. Aiming for 99% of the requests to be under a threshold gave too much variability while 95% was fairly stable."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#experiment",
    "href": "posts/gcp-serverless-comparison/index.html#experiment",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Experiment",
    "text": "Experiment\nThe source code is available here: project setup and services creation code\nSteps:\n\nEach of the serverless services is loaded with, basically, the same “hello world” app. The services need slightly different boilerplate code, but ultimately the code we are testing is the same.\nEach of the serverless services is set to 1 CPU, 1 concurrency, 1 max instance, and around 2 GB of memory.\nEach service is hit a few times to make sure the service is up and ready.\n“Hey” is used to generate 5,000 requests with some amount of concurrent requests. The number of overall requests was picked to minimize outlier request responses.\nThe amount of concurrent requests from “Hey” is changed until 95% of the request responses are under 100 ms.\nOnce a good result is achieved, the test is run 3 or 4 times to make sure the result is stable and then the best stable result is taken."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#experiment-results",
    "href": "posts/gcp-serverless-comparison/index.html#experiment-results",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Experiment Results",
    "text": "Experiment Results\nI’ve run these experiments twice: once six months ago and once again this week. The results were similar during both runs with the exception of Cloud Functions 1st Gen doing a bit better this time than the first time I ran the experiment.\n\nApp Engine\n\n\n\nApp Engine Results\n\n\nApp Engine was able to achieve ~340 requests per second with a Hey concurrency of 20 requests at a time.\n\n\nCloud Functions 1st Gen\n\n\n\nCloud Functions 1st Gen Results\n\n\nCloud Functions 1st Gen was able to achieve ~300 requests per second with a Hey concurrency of 20 requests at a time.\n\n\nCloud Functions 2nd Gen\n\n\n\nCloud Functions 2nd Gen Results\n\n\nCloud Functions 2nd Gen was able to achieve ~320 requests per second with a Hey concurrency of 20 requests at a time.\n\n\nCloud Run\n\n\n\nCloud Run Results\n\n\nCloud Run was able to achieve ~440 requests per second with a Hey concurrency of 25 requests at a time.\n\n\nNormalized Results\nCloud Run was able to give the best request throughput, so taking that as the index gives the following results:\n\nService Request Throughput Results Indexed To The Top Performer\n\n\nService\nRequests/sec\n% of Index\n\n\n\n\nCloud Run\n441\n100.0%\n\n\nApp Engine\n341\n77.3%\n\n\nCloud Functions 2nd Gen\n318\n72.1%\n\n\nCloud Functions 1st Gen\n298\n67.6%\n\n\n\nThese normalized results should be more helpful in determining relative throughput between the different services than the actual requests per second values. While the distance to the cloud data center or a different programming language might change the real throughput, the relative throughput should give a good idea of what to expect between the services.\n\n\nVerifying Results\n\nResults From Another Location\nI reran the experiment from another state in the U.S. to the same data center and got similar results.\nCloud Functions 1st Gen had the most variability (~242 requests/sec to begin with) before stabilizing around a similar response throughput as the above results.\n\nService Request Throughput Results Indexed To The Top Performer\n\n\nService\nRequests/sec\n% of Index\n\n\n\n\nCloud Run\n452\n100.0%\n\n\nApp Engine\n354\n78.3%\n\n\nCloud Functions 2nd Gen\n336\n74.3%\n\n\nCloud Functions 1st Gen\n304\n67.3%\n\n\n\n\n\nExtra App Concurrency\nI did try allowing the services to handle extra concurrency but the app would thrash between handling incoming requests and handling the current request (nodejs is single threaded). Overall this just slowed down the app request throughput.\n\n\nMore CPU\nI also tested adding more CPUs to the services to see if that had any unexpected effects, but that didn’t change the result."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#developer-experience",
    "href": "posts/gcp-serverless-comparison/index.html#developer-experience",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Developer Experience",
    "text": "Developer Experience\n\nApp Engine\nApp Engine gave the worst developer experience of the four services:\n\nTo enable App Engine, you must have “project owner” permissions.\nThe service can only be enabled once per project and in only one region. To enable the service in more than one region requires a project to be created per region.\nThe first app that is created in App Engine has to be called “default”. And this app can’t be deleted after creation, so Terraform can’t clean this up.\nApp Engine can’t be turned off or deleted after it is turned on in a project. The project has to be deleted to clean up App Engine.\nIn Terraform, you must include the runtime and entrypoint even though they are already defined in the app’s app.yaml file.\nIn Terraform, you have to manually version the app and this is difficult to automate.\nIn Terraform, there isn’t a way to get the app url. The app url has to be manually calculated with the App Engine’s location code being hardcoded.\n\nSome other notes about App Engine that are more neutral:\n\nThe apps are public to the internet by default. The other services (Cloud Functions and Cloud Run) had to have a role binding applied to allow anonymous access.\nGoogle has a comment in the App Engine vs Cloud Run comparison document that says they took what they learned from App Engine and put it into Cloud Run. And, that they made Cloud Run integrate with more services than with App Engine.\n\n\n\nCloud Functions\nThe Cloud Functions developer experience was pretty much exactly as expected (which is good). The following notes are things that stood out.\nCloud Functions 1st Gen:\n\nThere isn’t a way to control the CPU count directly. The CPU count is controlled by the amount of memory.\nEach function can only handle 1 concurrent request at a time.\n\nCloud Functions 2nd Gen:\n\nThe 2nd generation of Cloud Functions fixed the problems with the 1st gen.\nThe 2nd gen is actually using Cloud Run behind the scenes.\n\nCloud Functions (both generations) do have a potential negative, but I didn’t run into it during this experiment:\n\nEach function can only have one trigger. Multiple functions would have to be created to handle different triggering events even if the code to run was the same for each event.\n\n\n\nCloud Run\nCloud Run had the best developer experience. Since this service just runs a container, there is no special code or language required.\nThe only notes I would add deal with how the container is created and loaded into the Cloud Run service:\n\nThere is no way to submit code to Cloud Run directly from Terraform and no way to trigger Cloud Build to then submit the code to the service from Terraform. This typically wouldn’t be a problem in a normal development scenario because an external pipeline would be setup to maintain the service with the correct code. But, I didn’t want that overhead for these test experiments, so I had to create a script step to run the build and then deploy the container to Cloud Run after Terraform was applied.\nCloud Run can’t be created without an initial container image, and because of the specifics of this test environment, the initial container was set to a default Google container which was replaced later in a script step after Terraform was applied.\nCloud Build can’t be told where the Dockerfile is, so the app Dockerfile had to live in the root app folder instead of in a devops folder or somewhere more appropriate. There is a way to specify Dockerfile locations with a cloudbuild.yaml file, but this requires including special code (the cloudbuild.yaml file) which wasn’t appealing."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#conclusion",
    "href": "posts/gcp-serverless-comparison/index.html#conclusion",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Conclusion",
    "text": "Conclusion\nCloud Run has the best request throughput and developer experience of the different services. Cloud Functions (2nd gen) is using Cloud Run behind the scenes, and App Engine was taken as a lesson while building Cloud Run.\nBased on these findings, Cloud Run is the clear favorite."
  },
  {
    "objectID": "posts/gcp-serverless-comparison/index.html#comments",
    "href": "posts/gcp-serverless-comparison/index.html#comments",
    "title": "GCP Serverless Web Services Comparison",
    "section": "Comments",
    "text": "Comments\nLeave a comment:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey there! I’m Casey, and this is a blog about computers and other things I happen to write about."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud Report",
    "section": "",
    "text": "GCP Serverless Web Services Comparison\n\n\n\n\n\n\n\ngcp\n\n\nserverless\n\n\ncloud functions\n\n\ncloud run\n\n\napp engine\n\n\n\n\nA comparison between App Engine, Cloud Functions (1st and 2nd gen), and Cloud Run on Google Cloud.\n\n\n\n\n\n\nNov 28, 2023\n\n\nCasey Haralson\n\n\n\n\n\n\nNo matching items"
  }
]